{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Introduction to Natural Language Processing (NLP)\n### NLP is a field of artificial intelligence that helps computers understand, interpret, and manipulate human language.\n\n### In this notebook, we will use a scene from *The Shawshank Redemption* to demonstrate various NLP techniques.\n## Our goals are to:\n### - Preprocess the text (cleaning and preparing)\n### - Represent the text in numerical formats using different techniques\n### - Understand how these representations can be used in machine learning\n","metadata":{}},{"cell_type":"markdown","source":"1. **Text Representation and Preprocessing:**\n\n   This involves transforming text into a format that can be effectively analyzed by machine learning algorithms. Text data is inherently unstructured and requires preprocessing steps like tokenization, stop word removal, stemming, and lemmatization to standardize and clean it. Preprocessing converts text into a more uniform, structured format that can be used for machine learning tasks.\n\n2. **Introduction to NLP and Text Processing:**\n\n   Natural Language Processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and human languages. NLP aims to read, decipher, understand, and make sense of human languages in a valuable way. The process involves various techniques, including syntactic and semantic analysis, sentiment analysis, and named entity recognition, to interpret and analyze the text data.\n\n3. **Text Preprocessing: Tokenization, Stop Word Removal, Stemming, and Lemmatization:**\n\n   - **Tokenization**: The process of breaking down text into smaller units, such as words or sentences. It helps in splitting text into meaningful elements that can be analyzed. For example, \"I've seen that magazine before\" can be tokenized into [\"I've\", \"seen\", \"that\", \"magazine\", \"before\"].\n   \n   - **Stop Word Removal**: This involves removing common words (such as \"the\", \"is\", \"in\") that do not contribute much to the text's meaning in the context of the analysis. Removing stop words reduces noise and helps focus on the important words.\n   \n   - **Stemming**: This reduces words to their root form by chopping off prefixes or suffixes. Stemming is useful for normalizing words to their base form, but it often results in words that are not valid. For example, \"running\" becomes \"run\", and \"magazines\" becomes \"magazin\".\n   \n   - **Lemmatization**: Similar to stemming but more sophisticated. Lemmatization reduces words to their base or dictionary form while considering the context in which they are used. It produces valid words. For example, \"better\" can be lemmatized to \"good\" depending on the context, and \"running\" becomes \"run\".\n\n4. **Different Text Vectorization Techniques: Bag of Words (BoW), TF-IDF, and N-grams:**\n\n   - **Bag of Words (BoW)**: Represents text data as a collection of individual words, without regard to grammar or word order. It creates a vocabulary from all unique words in the text and represents each document as a vector that counts the occurrences of each word in the document.\n   \n   - **TF-IDF (Term Frequency-Inverse Document Frequency)**: A statistical measure used to evaluate how important a word is to a document in a collection or corpus. It considers the frequency of a word in a document (TF) and the inverse frequency of the word across all documents (IDF). This technique highlights words that are frequent in a document but not in others.\n   \n   - **N-grams**: Represents sequences of words in a text. \"Uni-gram\" refers to a single word, \"bi-gram\" to a sequence of two words, and \"tri-gram\" to a sequence of three words. N-grams are used to capture context or word relationships, making it useful for language models and sentiment analysis.","metadata":{}},{"cell_type":"code","source":"scene_text = \"\"\"\nRed: I've seen that magazine before. It's got a story about a guy who found a treasure map.\nAndy: Yeah, I've read it too.\nRed: It's a load of crap.\nAndy: You never know, Red.\nRed: Yeah, well, I'm not holding my breath.\nAndy: Ever think about what you'd do if you found a treasure?\nRed: Yeah, I guess I have.\nAndy: Where would you go?\nRed: Somewhere warm. Maybe Mexico.\nAndy: That sounds nice.\nRed: What about you, Andy? Where would you go?\nAndy: I'd go to the beach. Just sit there and listen to the waves.\nRed: The beach?\nAndy: Yeah. The beach.\n\"\"\"\n\nprint(scene_text)","metadata":{"execution":{"iopub.status.busy":"2024-08-26T17:43:39.941541Z","iopub.execute_input":"2024-08-26T17:43:39.942290Z","iopub.status.idle":"2024-08-26T17:43:39.948119Z","shell.execute_reply.started":"2024-08-26T17:43:39.942243Z","shell.execute_reply":"2024-08-26T17:43:39.947057Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"\nRed: I've seen that magazine before. It's got a story about a guy who found a treasure map.\nAndy: Yeah, I've read it too.\nRed: It's a load of crap.\nAndy: You never know, Red.\nRed: Yeah, well, I'm not holding my breath.\nAndy: Ever think about what you'd do if you found a treasure?\nRed: Yeah, I guess I have.\nAndy: Where would you go?\nRed: Somewhere warm. Maybe Mexico.\nAndy: That sounds nice.\nRed: What about you, Andy? Where would you go?\nAndy: I'd go to the beach. Just sit there and listen to the waves.\nRed: The beach?\nAndy: Yeah. The beach.\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Install NLTK if not already installed","metadata":{}},{"cell_type":"code","source":"!pip install nltk","metadata":{"execution":{"iopub.status.busy":"2024-08-26T17:43:39.950128Z","iopub.execute_input":"2024-08-26T17:43:39.950461Z","iopub.status.idle":"2024-08-26T17:43:52.176197Z","shell.execute_reply.started":"2024-08-26T17:43:39.950427Z","shell.execute_reply":"2024-08-26T17:43:52.174848Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Importing necessary libraries for text processing","metadata":{}},{"cell_type":"code","source":"import nltk\nimport os\nimport zipfile \n\nnltk_data_dir = '/root/nltk_data'\nif not os.path.exists(nltk_data_dir):\n    os.makedirs(nltk_data_dir)\n\nnltk.download('punkt', download_dir=nltk_data_dir)\nnltk.download('stopwords', download_dir=nltk_data_dir)\nnltk.download('wordnet', download_dir=nltk_data_dir)\nnltk.download('omw-1.4', download_dir=nltk_data_dir)\n\nwordnet_zip_path = os.path.join(nltk_data_dir, 'corpora', 'wordnet.zip')\nwordnet_extracted_path = os.path.join(nltk_data_dir, 'corpora', 'wordnet')\n\nif not os.path.exists(wordnet_extracted_path):\n    print(\"Extracting 'wordnet.zip'...\")\n    with zipfile.ZipFile(wordnet_zip_path, 'r') as zip_ref:\n        zip_ref.extractall(os.path.join(nltk_data_dir, 'corpora'))\n\nnltk.data.path.append(nltk_data_dir)","metadata":{"execution":{"iopub.status.busy":"2024-08-26T17:43:52.178141Z","iopub.execute_input":"2024-08-26T17:43:52.179212Z","iopub.status.idle":"2024-08-26T17:43:52.188751Z","shell.execute_reply.started":"2024-08-26T17:43:52.179158Z","shell.execute_reply":"2024-08-26T17:43:52.187729Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /root/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /root/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nimport string","metadata":{"execution":{"iopub.status.busy":"2024-08-26T17:43:52.190227Z","iopub.execute_input":"2024-08-26T17:43:52.190587Z","iopub.status.idle":"2024-08-26T17:43:52.200075Z","shell.execute_reply.started":"2024-08-26T17:43:52.190553Z","shell.execute_reply":"2024-08-26T17:43:52.199216Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"### Download required NLTK data files","metadata":{}},{"cell_type":"markdown","source":"### Display the original scene text","metadata":{}},{"cell_type":"code","source":"print(\"Original Scene Text:\\n\", scene_text)","metadata":{"execution":{"iopub.status.busy":"2024-08-26T17:43:52.202405Z","iopub.execute_input":"2024-08-26T17:43:52.202728Z","iopub.status.idle":"2024-08-26T17:43:52.215289Z","shell.execute_reply.started":"2024-08-26T17:43:52.202695Z","shell.execute_reply":"2024-08-26T17:43:52.214251Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Original Scene Text:\n \nRed: I've seen that magazine before. It's got a story about a guy who found a treasure map.\nAndy: Yeah, I've read it too.\nRed: It's a load of crap.\nAndy: You never know, Red.\nRed: Yeah, well, I'm not holding my breath.\nAndy: Ever think about what you'd do if you found a treasure?\nRed: Yeah, I guess I have.\nAndy: Where would you go?\nRed: Somewhere warm. Maybe Mexico.\nAndy: That sounds nice.\nRed: What about you, Andy? Where would you go?\nAndy: I'd go to the beach. Just sit there and listen to the waves.\nRed: The beach?\nAndy: Yeah. The beach.\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Step 1: Tokenization\n### Tokenization is the process of breaking down text into smaller pieces, usually words or sentences.\n","metadata":{}},{"cell_type":"code","source":"tokens = word_tokenize(scene_text)\nprint(\"\\nTokens:\\n\", tokens)","metadata":{"execution":{"iopub.status.busy":"2024-08-26T17:43:52.216599Z","iopub.execute_input":"2024-08-26T17:43:52.216974Z","iopub.status.idle":"2024-08-26T17:43:52.231364Z","shell.execute_reply.started":"2024-08-26T17:43:52.216913Z","shell.execute_reply":"2024-08-26T17:43:52.230312Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"\nTokens:\n ['Red', ':', 'I', \"'ve\", 'seen', 'that', 'magazine', 'before', '.', 'It', \"'s\", 'got', 'a', 'story', 'about', 'a', 'guy', 'who', 'found', 'a', 'treasure', 'map', '.', 'Andy', ':', 'Yeah', ',', 'I', \"'ve\", 'read', 'it', 'too', '.', 'Red', ':', 'It', \"'s\", 'a', 'load', 'of', 'crap', '.', 'Andy', ':', 'You', 'never', 'know', ',', 'Red', '.', 'Red', ':', 'Yeah', ',', 'well', ',', 'I', \"'m\", 'not', 'holding', 'my', 'breath', '.', 'Andy', ':', 'Ever', 'think', 'about', 'what', 'you', \"'d\", 'do', 'if', 'you', 'found', 'a', 'treasure', '?', 'Red', ':', 'Yeah', ',', 'I', 'guess', 'I', 'have', '.', 'Andy', ':', 'Where', 'would', 'you', 'go', '?', 'Red', ':', 'Somewhere', 'warm', '.', 'Maybe', 'Mexico', '.', 'Andy', ':', 'That', 'sounds', 'nice', '.', 'Red', ':', 'What', 'about', 'you', ',', 'Andy', '?', 'Where', 'would', 'you', 'go', '?', 'Andy', ':', 'I', \"'d\", 'go', 'to', 'the', 'beach', '.', 'Just', 'sit', 'there', 'and', 'listen', 'to', 'the', 'waves', '.', 'Red', ':', 'The', 'beach', '?', 'Andy', ':', 'Yeah', '.', 'The', 'beach', '.']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Step 2: Stop Word Removal\n### Stop words are common words (like \"the\", \"is\", \"in\") that are usually removed in text processing","metadata":{}},{"cell_type":"code","source":"stop_words = set(stopwords.words('english'))\nfiltered_tokens = [word for word in tokens if word.lower() not in stop_words]\nprint(\"\\nFiltered Tokens (Stop Words Removed):\\n\", filtered_tokens)","metadata":{"execution":{"iopub.status.busy":"2024-08-26T17:43:52.232690Z","iopub.execute_input":"2024-08-26T17:43:52.233231Z","iopub.status.idle":"2024-08-26T17:43:52.243022Z","shell.execute_reply.started":"2024-08-26T17:43:52.233184Z","shell.execute_reply":"2024-08-26T17:43:52.242105Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"\nFiltered Tokens (Stop Words Removed):\n ['Red', ':', \"'ve\", 'seen', 'magazine', '.', \"'s\", 'got', 'story', 'guy', 'found', 'treasure', 'map', '.', 'Andy', ':', 'Yeah', ',', \"'ve\", 'read', '.', 'Red', ':', \"'s\", 'load', 'crap', '.', 'Andy', ':', 'never', 'know', ',', 'Red', '.', 'Red', ':', 'Yeah', ',', 'well', ',', \"'m\", 'holding', 'breath', '.', 'Andy', ':', 'Ever', 'think', \"'d\", 'found', 'treasure', '?', 'Red', ':', 'Yeah', ',', 'guess', '.', 'Andy', ':', 'would', 'go', '?', 'Red', ':', 'Somewhere', 'warm', '.', 'Maybe', 'Mexico', '.', 'Andy', ':', 'sounds', 'nice', '.', 'Red', ':', ',', 'Andy', '?', 'would', 'go', '?', 'Andy', ':', \"'d\", 'go', 'beach', '.', 'sit', 'listen', 'waves', '.', 'Red', ':', 'beach', '?', 'Andy', ':', 'Yeah', '.', 'beach', '.']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Step 3: Stemming\n### Stemming reduces words to their root form (e.g., \"running\" to \"run\")","metadata":{}},{"cell_type":"code","source":"stemmer = PorterStemmer()\nstemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\nprint(\"\\nStemmed Tokens:\\n\", stemmed_tokens)","metadata":{"execution":{"iopub.status.busy":"2024-08-26T17:43:52.244248Z","iopub.execute_input":"2024-08-26T17:43:52.244649Z","iopub.status.idle":"2024-08-26T17:43:52.254385Z","shell.execute_reply.started":"2024-08-26T17:43:52.244606Z","shell.execute_reply":"2024-08-26T17:43:52.253403Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"\nStemmed Tokens:\n ['red', ':', \"'ve\", 'seen', 'magazin', '.', \"'s\", 'got', 'stori', 'guy', 'found', 'treasur', 'map', '.', 'andi', ':', 'yeah', ',', \"'ve\", 'read', '.', 'red', ':', \"'s\", 'load', 'crap', '.', 'andi', ':', 'never', 'know', ',', 'red', '.', 'red', ':', 'yeah', ',', 'well', ',', \"'m\", 'hold', 'breath', '.', 'andi', ':', 'ever', 'think', \"'d\", 'found', 'treasur', '?', 'red', ':', 'yeah', ',', 'guess', '.', 'andi', ':', 'would', 'go', '?', 'red', ':', 'somewher', 'warm', '.', 'mayb', 'mexico', '.', 'andi', ':', 'sound', 'nice', '.', 'red', ':', ',', 'andi', '?', 'would', 'go', '?', 'andi', ':', \"'d\", 'go', 'beach', '.', 'sit', 'listen', 'wave', '.', 'red', ':', 'beach', '?', 'andi', ':', 'yeah', '.', 'beach', '.']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Step 4: Lemmatization\n### Lemmatization reduces words to their base or dictionary form (e.g., \"running\" to \"run\"), but is more accurate than stemming","metadata":{}},{"cell_type":"code","source":"lemmatizer = WordNetLemmatizer()\nlemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\nprint(\"\\nLemmatized Tokens:\\n\", lemmatized_tokens)","metadata":{"execution":{"iopub.status.busy":"2024-08-26T17:43:52.255581Z","iopub.execute_input":"2024-08-26T17:43:52.255912Z","iopub.status.idle":"2024-08-26T17:43:52.265833Z","shell.execute_reply.started":"2024-08-26T17:43:52.255875Z","shell.execute_reply":"2024-08-26T17:43:52.264720Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"\nLemmatized Tokens:\n ['Red', ':', \"'ve\", 'seen', 'magazine', '.', \"'s\", 'got', 'story', 'guy', 'found', 'treasure', 'map', '.', 'Andy', ':', 'Yeah', ',', \"'ve\", 'read', '.', 'Red', ':', \"'s\", 'load', 'crap', '.', 'Andy', ':', 'never', 'know', ',', 'Red', '.', 'Red', ':', 'Yeah', ',', 'well', ',', \"'m\", 'holding', 'breath', '.', 'Andy', ':', 'Ever', 'think', \"'d\", 'found', 'treasure', '?', 'Red', ':', 'Yeah', ',', 'guess', '.', 'Andy', ':', 'would', 'go', '?', 'Red', ':', 'Somewhere', 'warm', '.', 'Maybe', 'Mexico', '.', 'Andy', ':', 'sound', 'nice', '.', 'Red', ':', ',', 'Andy', '?', 'would', 'go', '?', 'Andy', ':', \"'d\", 'go', 'beach', '.', 'sit', 'listen', 'wave', '.', 'Red', ':', 'beach', '?', 'Andy', ':', 'Yeah', '.', 'beach', '.']\n","output_type":"stream"}]}]}